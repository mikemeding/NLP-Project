Intro to Machine learning
Since we are looking for potentially bullying tweets we need a way of classifying a tweet as bullying or not bullying. The method that we decided on was to make use a machine learning algorithm called artificial neural networks. Artificial neural networks were inspired by biological neural networks and are used to approximate functions. They take in some input and produce an output. Each input is effectively a neuron, and when these neurons are activated they pass information to another set of neurons. Like other machine learning algorithms an artificial neural network can learn from data. They are part of a class of machine learning algorithms called supervised learning algorithms, which means that we need more than raw data, the data must be annotated. The algorithm will take the data and learn how to map the input to the correct output. In our case it will learn how to take a tweet and classify it as being bullying or not bullying.

As with any function, bad inputs produce bad outputs. Tweets in their raw form aren't exactly machine readable. We need to translate it into something our artificial neural network can use as input. What we need to do is extract features from a tweet so that we can feed them to the learning algorithm. Aside from making the input we also have to decide on the configuration of the network and how it learns. Finally we needed to test our artificial neural network to make sure it works on data that it has never seen to see if it was indeed learn what bullying tweets look like instead of over fitting our training data and only being able to classify things that look exactly like that.

Ark Tweet NLP
We crafted a number of features with the help of Ark Tweet NLP, SentiWordNet, the Harvard General Inquirer, and the Subjectivity Lexicon from the University of Pittsburgh. Ark Tweet NLP is a project from Carnegie Mellon that provides a tokenizer, a part-of-speech tagger, hierarchical word clusters, and a dependency parser for tweets. SentiWordNet is a lexical resource which contains a number of entries with three sentiment scores (Positive, Negative, and Objective). The Harvard General Inquirer is another lexical resource but instead of scores each entry has a number of categories it can be a member of. The Subjectivity Lexicon contains entries and provide the strength of the subjectivity and polarity.

What we wanted to do was classify a tweet as bullying or not bullying by looking at word sense and the structure of the tweet rather than the words that were actually in them. One of the first things we needed to do in order to extract features from a tweet is to tokenize and get the part-of-speech for each token. Ark Tweet NLP was created to handle tweets and has twitter specific tags for things such as hashtags, at-mentions, and emoticons among other things. The tokens along with their tags become the basis for just about every feature that we built.

SentiWordNet
Initially we had planned to find bullying tweets solely with sentiment analysis, so our first features made use of SentiWordNet. Each SentiWordNet entry contains five explicit attributes (part-of-speech, id, positive score, negative score, synset terms, and glossary) and an implicit attribute (objective which is 1 - positive score - negative score). A word may have multiple entries. Since we have no way of disambiguating entries we went with a naive approach and added up all scores where the entries contained the word and had the same part-of-speech tag.

The first few features we implemented were scores divided by entry count and if there are more negative entries than positive entries. Unfortunately those features by themselves were not able to classify a tweet as being bullying or not bullying. Then we came across a paper called
 Opinion Mining Using SentiWordNet by Julia Kreutzer and Neele Witte. (reference needed)
In the paper they made a number of interesting points. One thing they did was average by using non-zero score counts. Another thing in the paper we took note of was the claim "adjectives are the group of words that carry the most notions of sentiment" which appears to make a lot of sense. Also they made use of majority counts. So we implemented scores divided by non-zero score entry count, scores for adjectives divided by non-zero score entry count, and binary features for majority counts. Since we figured objectivity would usually have the majority we added two more binary that check if there are more negative counts than positive counts and the adjective only version of it.

Using sentiment scores only gave us a hand full of features and those features by themselves were nowhere near enough to help use classify tweets as bullying or not. We knew that we needed more features. Feeling like we had exhausted what we could from sentiment scores we looked for other potential lexical resource.

Harvard Inquirer
The next set of features we created leveraged the Harvard General Inquirer. This lexical resource does not provide scores but it did mark which categories an entry belonged to. So the features that we implemented using this resource was a number of binary features that would say if the entire tweet contained a word that was contained in a specific category. The General inquirer has 182 categories so we picked out a few categories that might be able to classify tweets that are bullying and not bullying. We started with positive, negative, hostile, fail, active, and passive. These features seemed to help discriminate tweets so we continued to use more categories until we simply decided to use all the categories.

After that we took a look at the Subjectivity Lexicon which gives the strength of the subjectivity and polarity for each of its entries. There were two values for strength (strong and weak) and two values for polarity (positive and negative) which means there are four permutations. With that we created two sets of binary features, one set looks at all words in a tweet and the other set looks at only adjectives. A set of binary features were made to compare the number of negative polarity counts to positive polarity counts and another was designed to compare the total number of weak counts to strong counts and negative counts to positive counts.

Pattern recognition
We created set of features that looked at the patterns of part-of-speech tags in the tweets. We discovered the patterns by using n-grams, specifically bigrams and trigrams. Next we calculated how often a pattern appeared in both the bullying and non-bullying tweets and decided to keep patterns that showed up more than ten percent of the time in either the bullying or non-bullying tweets. This turned into a large number of binary features that shows whether or not a specific pattern exists in a tweet or not.

Machine Learning
While we were designing our features we were tweaking the configuration of our artificial neural network. We chose to go with an artificial neural network over other machine learning algorithms, such as support vector machines, because of its flexibility. Instead of building the artificial neural network ourselves we used a library called Encog Machine Learning Framework. 

At first the network was configured to have two layers, input and output, and we quickly found out that was not enough to classify our dataset. Using our full set of features the two layer configuration was able to achieve high precision but low recall. Even before splitting the data into training and testing we knew that this configuration would not work. Next we added a hidden layer with one hundred nodes. The precision dipped a bit and recall went up a little bit. Then we changed it to two hundred nodes and both precision and recall were similar to one hundred nodes but it took significantly longer to train. Finally we added another layer with one hundred nodes and we got extremely high precision and recall. So this is the configuration we decided to stick with. While we were trying different configurations we also tried two different training methods. The methods were backpropagation and resilient propagation. Both had similar results but resilient propagation appeared to converge faster than backpropagation.

Testing, unfortunately, did not go nearly as well as we had hoped. Since our dataset isn't very large we had to segment our data into a training set and a testing set. We trying using a few different distributions making sure we had the same ratio or bullying and not bullying in both data sets. In each case the results were quite similar, low precision and low recall. The only bright point is that, despite its poor performance, it is still better than randomly guessing. Clearly it is over fitting it's training data. If we had more bullying tweets we might be able appropriately classify them.
